\documentclass[a4paper,11pt]{article}

\usepackage[pdftex]{graphicx}
%\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[T1,mtbold,lucidacal,mtplusscr,subscriptcorrection]{mathtime}
\usepackage{times}
\usepackage{amsmath}
\usepackage{url}
\usepackage{enumerate}
\usepackage{parskip}
\usepackage[colorlinks,urlcolor=black]{hyperref}
\usepackage{microtype}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{listings}
\usepackage{pythonhighlight}
 \usepackage{relsize}

% if not draft, smaller printable area makes the paper more readable
\topmargin -4mm
\oddsidemargin 0mm
\textheight 225mm
\textwidth 150mm

%\parskip=\baselineskip

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\Sd}{Sd}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\betacdf}{betacdf}
\DeclareMathOperator{\Invchi2}{Inv-\chi^2}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\trace}{trace}
\newcommand{\vc}[1] { \mathbf{#1} }
\newcommand{\vs}[1] { \boldsymbol{#1} }

\pagestyle{empty}

\begin{document}
\thispagestyle{empty}

\section*{Naive Bayes Classifier}


The purpose of this is to test how well the naive Bayes model fits the data in contrast to the one used in the research paper.

But first we need to define our model. A Bernoulli naive Bayes is good for the purpose: it uses binary data to sort samples into categories; in this case two. The model has the following structure

\[ p(C_i \mid {\bf x} ) \propto p({\bf x} \mid C_i ) p(C_i) = p(C_i)\mathlarger{\mathlarger{ \Pi }}_{j=1}^{N} p(x_j \mid C_i)^{b_j}(1 - p(x_j \mid C_i))^{(1-b_j)} \]

where $b_j$ denotes the boolean value of having the TCR of the index $j$. 

Moreover, these probabilities are based on the expected values of the proportions in the sample. This means that $p(x_j \mid C_i)$ is actually $p(x_j \mid C_i, {\bf x}_0)$ in this case. Because they are modelled as fixed probabilities in each group, these underlying probabilities are Beta distributed. 

Due to the independence assumption, we can first calculate the expected values of the probabilites, and then calculate the product for class probability estimation.

The proportional values are transformed into probabilities

\[ p(C_i \mid {\bf x} ) = \frac{p({\bf x} \mid C_i ) p(C_i) }{ \Sigma_{n=1}^K p({\bf x} \mid C_n ) p(C_n)} \]

where $K$ is the number of classes, in this case 2.

{\bf Source:}

\begin{python}
# here python code if necessary
\end{python}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: